{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写数字的图像识别，input data是28x28黑白图片，用LR建模\n",
    "\n",
    "LR 用于计算input属于某个target class 的概率，本例中是[0-9]\n",
    "![Logistic Regression classifier expression](LR_classifier_expression.png)\n",
    "**我们的目标是找出最优的 W 和 b**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression network\n",
    "![Logistic Regression network](LR_network.png)\n",
    "这个LR网络的图片解释相当简陋。它没有隐层，意味它学习复杂关系的能力有限。\n",
    "input layer 有28x28=784个neurons，每个像素对应一个neuron。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR建模4个阶段\n",
    "    1. inference: 给定一个minibatch（inputs），生成其output class 的概率分布\n",
    "    2. loss: 计算 error function 的值，本例中采用cross-entropy loss\n",
    "    3. training: 计算参数的梯度并更新模型（函数表达式），即W,b 的梯度下降\n",
    "    4. evaluate: 评估一个模型的有效性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. inference: 给定一个minibatch，生成其output class 的概率分布\n",
    "给定一个minibatch，它由784维向量组成，每个向量表示一张28x28的image。\n",
    "logistic regression 模型的表达式为![Logistic Regression classifier expression](LR_classifier_expression.png)\n",
    "**x**是input，矩阵W表示输入和输出层之间的连接权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", [784, 10], initializer=init)\n",
    "    b = tf.get_variable('b', [10], initializer=init)\n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)  # 输出是一个概率值P\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. loss: 计算 error function 的值，本例中采用cross-entropy loss\n",
    "接下来，给出minibatch 的真实labels **y**，与outputs比较 以计算每个数据样本的平均误差。\n",
    "\n",
    "cross-entropy ≠ loss function。设p是真label的概率，q是预测的概率，交叉熵度量p和q的相似度（距离）：\n",
    "![cross-entropy](cross_entropy.svg)\n",
    "\n",
    "Log loss =  logistic loss = cross-entropy loss：\n",
    "\n",
    "![log loss](cross_entropy_loss.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    dot_product = y * tf.log(output)\n",
    "    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1) # 沿轴0缩减是每column折叠为单个值，轴1是每row；\n",
    "    loss = tf.reduce_mean(xentropy)                                # 通常沿轴i 即将向量的第i维折叠为单个值。\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. training: 计算参数的梯度并更新模型（函数表达式），即W,b 的梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了cost function之后，接下来计算梯度并优化模型的参数。注意对已迭代minibatch 进行计数的参数global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) # learning_rate 在函数外自定义\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. evaluate: 评估一个模型的有效性\n",
    "\n",
    "最后组装一个computational subgraph 来评估模型在validation 或 test set 上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    correct_predicton = tf.equal(tf.argmax(output, 1),\n",
    "                                 tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是LR模型的TensorFlow graph setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 训练模型并记录日志"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型时，为了log 重要信息，我们要将几个概要统计量（summary statistics）记入日志。\n",
    "\n",
    "例如用***tf.scalar_summary*** 和 ***tf.histogram_summary***  to\n",
    "\n",
    "    log the cost for each minibatch, validation error, and the distribution of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#例：cost function的标量摘要统计量\n",
    "def training(cost, global_step):\n",
    "    tf.scalar_summary('cost',cost)  # 比上面多了这一行\n",
    "    optimizer = tf.train.GrandientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个epoch， 我们可以用 ***tf.merge_all_summaries*** 来收藏所有已经记入日志的summary statistics，\n",
    "\n",
    "并可用***tf.train.SummaryWriter*** 将日志写入磁盘\n",
    "\n",
    "除了存储summary statistics，我们也可以用***tf.train.Saver***存储训练好的模型参数，以便未来调用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将以上整合一下，加入参数值，就是一个python 脚本了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "一张image里一位数字的识别，用logistic regression模型能达到91.1的准确率，即8.1%的错误率。\n",
    "\n",
    "貌似不错，但在高价值的实际应用上并不是很有用。\n",
    "\n",
    "比如，4位数的识别，1000-9999，那么错误率几乎是30%了。因此，我们接下来用feed-forward network 模型来试试看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 用TensorBoard可视化computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成，有了summary statistics的logging 之后，我们就可以将收集的数据进行可视化。\n",
    "\n",
    "     tensorboard --logdir=<absolute_path_to_log_dir>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
