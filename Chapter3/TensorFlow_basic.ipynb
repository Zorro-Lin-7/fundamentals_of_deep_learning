{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of a simple computational graph in TensorFlow\n",
    "![computational graph](computational_graph.png)\n",
    "\n",
    "f(**x**) = **x**W + **b**  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, name='x', shape=[None, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([784,10], -1, 1), name=\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.zeros([10]), name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating Variable Scopes and Sharing Variables\n",
    "\n",
    "构建复杂模型通常需要reuse and sharing大量的变量集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_network(input):\n",
    "    W_1 = tf.Variable(tf.random_uniform([784, 100], -1,1),name='W_1')\n",
    "    b_1 = tf.Variable(tf.zeros([100]), name='biases_1')\n",
    "    output_1 = tf.matmul(input, W_1) + b_1\n",
    "    \n",
    "    W_2 = tf.Variable(tf.random_uniform([100,50], -1, 1), name='W_2')  \n",
    "    b_2 = tf.Variable(tf.zeros([50]), name='biases_2')\n",
    "    output_2 = tf.matmul(output_1, W_2) + b_2    # reuse  output_1\n",
    "    \n",
    "    W_3 = tf.Variable(tf.random_uniform([50,10], -1, 1), name='W_3')\n",
    "    b_3 = tf.Variable(tf.zeros([10]), name='baises_3')\n",
    "    output_3 = tf.matmul(output_2, W_3) + b_3    # reuse output_2\n",
    "    \n",
    "    # printing names\n",
    "    print(\"Names of weight parameters\\n\")\n",
    "    print(W_1.name,'  ', W_2.name,'  ', W_3.name)\n",
    "    print(\"\\nNames of bias parameters\\n\")\n",
    "    print(b_1.name,'  ', b_2.name,'  ', b_3.name)\n",
    "    \n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是个network，3 layers，6 variables\n",
    "\n",
    "如果想多次使用这个network，就将它封装成function，以多次调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of weight parameters\n",
      "\n",
      "W_1:0    W_2:0    W_3:0\n",
      "\n",
      "Names of bias parameters\n",
      "\n",
      "biases_1:0    biases_2:0    baises_3:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_3:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of weight parameters\n",
      "\n",
      "W_1_1:0    W_2_1:0    W_3_1:0\n",
      "\n",
      "Names of bias parameters\n",
      "\n",
      "biases_1_1:0    biases_2_1:0    baises_3_1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_6:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_1 = tf.placeholder(tf.float32, [1000, 784], name='input_1')\n",
    "my_network(i_1)\n",
    "\n",
    "i_2 = tf.placeholder(tf.float32, [1000, 784], name='input_2')\n",
    "my_network(i_2)\n",
    "\n",
    "# i_1 和 i_2 两个变量是一样的（除了变量名不一样），但执行结果却不一样："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细观察会发现，第二个调用没有像第一个调用那样使用相同的变量\n",
    "\n",
    "在很多情况下，我们不想创建副本，而是重用模型及其变量。\n",
    "\n",
    "因此，本例中，我们不应使用tf.Variable，而应该使用一个更高级的命名scheme，它能利用tf 的variable scoping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 tf's variable scoping 重写my_network\n",
    "涉及2个functions:\n",
    "\n",
    "* tf.get_variable(&lt;name&gt;, &lt;shape&gt;, &lt;initializer&gt;)\n",
    "\n",
    "* tf.variable_scope(&lt;scope_name&gt;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_init = tf.random_uniform_initializer(minval=-1, maxval=1)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable('b', bias_shape, initializer=bias_init)\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def my_network(input):\n",
    "    with tf.variable_scope('layer_1'):\n",
    "        output_1 = layer(input, [784, 100], [100])\n",
    "    with tf.variable_scope('layer_2'):\n",
    "        output_2 = layer(output_1, [100, 50], [50])\n",
    "    with tf.variable_scope('layer_3'):\n",
    "        output_3 = layer(output_2, [50, 10], [10])\n",
    "        \n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_3/add:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_1 = tf.placeholder(tf.float32, [1000,784], name='input_1')\n",
    "my_network(i_1)\n",
    "\n",
    "i_2 = tf.placeholder(tf.float32, [1000,784], name='input_2')\n",
    "#my_network(i_2)  # 会报异常“已经存在”：Variable layer_1/W already exists，因为默认sharing 是不被允许的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 如果我们想在变量范围内启用共享:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shared_variables/layer_3/add:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shared_variables/layer_3_1/add:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"shared_variables\") as scope:\n",
    "    i_1 = tf.placeholder(tf.float32, [1000,784], name='i_1')\n",
    "    my_network(i_1)\n",
    "    scope.reuse_variables()\n",
    "    i_2 = tf.placeholder(tf.float32, [1000,784], name='i_2')\n",
    "    my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "这让我们保持模块化，同时允许变量共享。这也使命名方案更干净。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GPU VS CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若要检查运行的devices是哪个，则初始化session 时，设置参数log_device_placement=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "                        log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.],\n",
       "       [ 11.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):  # 指定使用某device；若该设备不可用，则会报错\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], name='a')\n",
    "    b = tf.constant([1.0, 2.0], shape=[2, 1], name='b')\n",
    "    c = tf.matmul(a, b) # 矩阵乘法运算\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "            allow_soft_placement=True, # 若选用的device不存在(如本机用'/gpu:0'的话),想另外找可用的，则传入该参数\n",
    "            log_device_placement=True))\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building multi-GPU models in a tower-like fashion\n",
    "![multi-GPU](multi-GPU_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.],\n",
       "       [ 22.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "for device in ['/gpu:0', '/gpu:1']:\n",
    "    with tf.device(device):\n",
    "        A = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], name='Matrix_A')\n",
    "        b = tf.constant([1.0, 2.0], shape=[2, 1], name='b')\n",
    "        c.append(tf.matmul(a, b))\n",
    "        \n",
    "with tf.device('/cpu:0'):\n",
    "    sum = tf.add_n(c)\n",
    "    \n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "                        allow_soft_placement=True,\n",
    "                        log_device_placement=True))\n",
    "sess.run(sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
